{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through a basic understanding of the working of RNN. You can have a lok at __[Chris Olah's blog on RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)__ to understand RNN architecture.\n",
    "\n",
    "We will be using a simple RNNfor which the cell-equation (not a standard name) is:\n",
    "\n",
    "$h_t = f(X \\times W + h_{t-1} \\times U + b)$\n",
    "\n",
    "\n",
    "**Setup**:<br>\n",
    "Our aim in first problem is to predict the sum of 3 numbers with RNN.\n",
    "Thus for each input sequence $[x_0, x_1, x_2]$, output should be $y = x_0 + x_1 + x_2$\n",
    "\n",
    "**Note**: I know the same can be achieved with a simple neural net, but to keep it simple we are setting the problem this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and model parameters\n",
    "seq_len = 3   #Length of each sequence \n",
    "rnn_size = 1  #Output shape of RNN\n",
    "input_size = 10000 #Numbers of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [9],\n",
       "        [3]],\n",
       "\n",
       "       [[2],\n",
       "        [1],\n",
       "        [8]],\n",
       "\n",
       "       [[4],\n",
       "        [4],\n",
       "        [6]],\n",
       "\n",
       "       [[8],\n",
       "        [6],\n",
       "        [1]],\n",
       "\n",
       "       [[8],\n",
       "        [7],\n",
       "        [8]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat = np.random.randint(low=0, high=10, size=(input_size,3,1))\n",
    "all_feat[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.],\n",
       "       [11.],\n",
       "       [14.],\n",
       "       [15.],\n",
       "       [23.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label = np.apply_along_axis(func1d=np.sum, axis=1, arr=all_feat)\n",
    "all_feat = all_feat.astype('float64') \n",
    "all_label = all_label.astype('float64') \n",
    "\n",
    "all_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "Our model will have only a Simple RNN.<br> \n",
    "Our expectation with RNN is that it will learn to pass the input as it is to next layer.<br>\n",
    "One more thing to note: to keep things simple to understand, we'll use linear activation($y = f(x) = x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer RNN_Layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     [(None, 3, 1)]            0         \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(3,1,), name='Input_Layer', dtype=tf.float64)\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 12s 1ms/sample - loss: 0.0096 - acc: 0.0035 - val_loss: 3.1774e-08 - val_acc: 0.0065\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 1.3047e-09 - acc: 0.0035 - val_loss: 2.0849e-12 - val_acc: 0.0065\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 1.0002e-12 - acc: 0.0035 - val_loss: 4.7439e-13 - val_acc: 0.0065\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 17s 2ms/sample - loss: 1.6407e-13 - acc: 0.0035 - val_loss: 1.1529e-19 - val_acc: 0.0065\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 22s 3ms/sample - loss: 1.6120e-20 - acc: 0.0035 - val_loss: 4.2912e-25 - val_acc: 0.0065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=all_feat, y=all_label, batch_size=4, epochs=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model looks fine. Let's check few predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[[1.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[3.]\n",
      "  [6.]\n",
      "  [3.]]\n",
      "\n",
      " [[6.]\n",
      "  [6.]\n",
      "  [7.]]\n",
      "\n",
      " [[1.]\n",
      "  [8.]\n",
      "  [0.]]\n",
      "\n",
      " [[4.]\n",
      "  [0.]\n",
      "  [8.]]]\n",
      "\n",
      "Labels: \n",
      " [[ 9.]\n",
      " [12.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [12.]]\n",
      "\n",
      "Predictions: \n",
      " [[ 9.]\n",
      " [12.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [12.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what RNN learnt. A little info on the RNN weight matrices:<br>\n",
    "There are three weights:\n",
    "1. W: Input to RNN weight Matrix\n",
    "2. U: RNN to RNN (or hidden layer to RNN) weight Matrix\n",
    "3. b: Bias matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_layer = model.get_layer('RNN_Layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]], dtype=float32),\n",
       " array([[1.]], dtype=float32),\n",
       " array([-5.6379376e-12], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights match the expectations. RNN equation is:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "As we have set $f$ to linear, the equations is\n",
    "\n",
    "$h_t = X \\times W +  h_{t-1} \\times U + b$\n",
    "\n",
    "We were expecting $W= 1, U = 1$ and $b = 0$, and the weights we got are quite close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to higher dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use one-hot encodings as the input to make the problem bit more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using keras preprocessing function\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_feat = np.apply_along_axis(func1d=lambda x: to_categorical(x,10), arr=all_feat, axis=1)\n",
    "all_cat_feat = all_cat_feat.reshape(all_feat.shape[0], 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.],\n",
       "        [9.],\n",
       "        [3.]],\n",
       "\n",
       "       [[2.],\n",
       "        [1.],\n",
       "        [8.]],\n",
       "\n",
       "       [[4.],\n",
       "        [4.],\n",
       "        [6.]],\n",
       "\n",
       "       [[8.],\n",
       "        [6.],\n",
       "        [1.]],\n",
       "\n",
       "       [[8.],\n",
       "        [7.],\n",
       "        [8.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cat_feat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating new model, we should delete the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer RNN_Layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     [(None, 3, 10)]           0         \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(3,10,), name='Input_Layer', dtype=tf.float64)\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "8000/8000 [==============================] - 17s 2ms/sample - loss: 49.4209 - acc: 0.0025 - val_loss: 21.1303 - val_acc: 0.0050\n",
      "Epoch 2/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.5492 - acc: 0.0025 - val_loss: 21.1913 - val_acc: 0.0050\n",
      "Epoch 3/8\n",
      "8000/8000 [==============================] - 11s 1ms/sample - loss: 20.5574 - acc: 0.0025 - val_loss: 21.2411 - val_acc: 0.0050\n",
      "Epoch 4/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.4887 - acc: 0.0025 - val_loss: 21.1229 - val_acc: 0.0050\n",
      "Epoch 5/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.4433 - acc: 0.0025 - val_loss: 21.0927 - val_acc: 0.0050\n",
      "Epoch 6/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.4263 - acc: 0.0025 - val_loss: 20.9746 - val_acc: 0.0050\n",
      "Epoch 7/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.3990 - acc: 0.0025 - val_loss: 21.2820 - val_acc: 0.0050\n",
      "Epoch 8/8\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 20.3678 - acc: 0.0025 - val_loss: 21.1165 - val_acc: 0.0050\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.005), loss='mean_squared_error', metrics=['acc'])\n",
    "history = model.fit(x=all_cat_feat, y=all_label, batch_size=8, epochs=8, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo may have noticed that I chaanges training paramters like learning rate, batch size etc. This is done to reach high accuracy.\n",
    "\n",
    "Let's check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[[1.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[3.]\n",
      "  [6.]\n",
      "  [3.]]\n",
      "\n",
      " [[6.]\n",
      "  [6.]\n",
      "  [7.]]\n",
      "\n",
      " [[1.]\n",
      "  [8.]\n",
      "  [0.]]\n",
      "\n",
      " [[4.]\n",
      "  [0.]\n",
      "  [8.]]]\n",
      "\n",
      "Labels: \n",
      " [[ 9.]\n",
      " [12.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [12.]]\n",
      "\n",
      "Predictions: \n",
      " [[11.216051]\n",
      " [11.630124]\n",
      " [14.002243]\n",
      " [10.156763]\n",
      " [14.664776]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_cat_feat[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time input dimenion is 10 and output dimension is still 1. Looking back at RNN equation:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "$W$ should have size $10 \\times 1$, while $U$ should still have size $1 \\times 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape:  (10, 1)\n",
      "U shape:  (1, 1)\n",
      "b shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "print('W shape: ', wgts_mats[0].shape)\n",
    "print('U shape: ', wgts_mats[1].shape)\n",
    "print('b shape: ', wgts_mats[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that W learns to transform one hot enocding to actual numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.3786769 ],\n",
       "        [0.4866051 ],\n",
       "        [0.52083737],\n",
       "        [0.57245713],\n",
       "        [0.72293156],\n",
       "        [0.7136882 ],\n",
       "        [0.7967396 ],\n",
       "        [0.8854931 ],\n",
       "        [0.9589181 ],\n",
       "        [1.0973808 ]], dtype=float32),\n",
       " array([[-3.0299761]], dtype=float32),\n",
       " array([1.1489911], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U$ looks alright, but $W$ seems somewhat different. Let me add $b$ to $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W+b: \n",
      " [[1.527668 ]\n",
      " [1.6355963]\n",
      " [1.6698284]\n",
      " [1.7214482]\n",
      " [1.8719227]\n",
      " [1.8626792]\n",
      " [1.9457307]\n",
      " [2.0344841]\n",
      " [2.1079092]\n",
      " [2.2463717]]\n",
      "\n",
      "U: \n",
      " [[-3.0299761]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nW+b: \\n', wgts_mats[0]+wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a much, much clear understanding, round the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W+b: \n",
      " [[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]]\n",
      "\n",
      "U: \n",
      " [[-3.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nW+b: \\n', np.round(wgts_mats[0]+wgts_mats[2]))\n",
    "print('\\nU: \\n', np.round(wgts_mats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our input vector $X$, which has only one 1 at the position given by input number, is multipled with $W$, it essentially gives out the value at same positions from the weight matrix $W$.\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} 0 & 0 & 1 & 0 \\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\\\ W_4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0 \\times W_1 \\\\ + \\\\ 0 \\times W_2 \\\\ + \\\\ 1 \\times W_3 \\\\ + \\\\ 0 \\times W_4 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} W_3 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\\begin{bmatrix} 0 & 0 & 1 & 0 \\end{bmatrix}$ is one-hot encoding for 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multitude of RNN models, you'll see embeddings beings used. Embedding are similar to one-hot encodings: An n-dimensional representation of your input(text generally) which learns the represetation along with the rest of the model.\n",
    "\n",
    "Here, We'll try to replace one-hot encodings with embeddings.\n",
    "\n",
    "Input will be numbers, need to be reshaped, and before the RNN layer, there will be an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_reshaped = all_feat.reshape(all_feat.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 3, 10)             100       \n",
      "_________________________________________________________________\n",
      "RNN_Layer (SimpleRNN)        (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 112\n",
      "Trainable params: 112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = Input(shape=(3,), name='Input_Layer')\n",
    "x = Embedding(input_dim=10, output_dim=10, name='Embedding_Layer')(input_1)\n",
    "y = SimpleRNN(rnn_size, activation='linear', name='RNN_Layer')(x)\n",
    "\n",
    "model = Model(inputs=input_1, outputs=y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 18s 2ms/sample - loss: 30.8264 - acc: 0.0025 - val_loss: 23.1295 - val_acc: 0.0050\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 11s 1ms/sample - loss: 22.0209 - acc: 0.0025 - val_loss: 22.6766 - val_acc: 0.0050\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 11s 1ms/sample - loss: 22.0165 - acc: 0.0025 - val_loss: 22.5417 - val_acc: 0.0050\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 11s 1ms/sample - loss: 21.5136 - acc: 0.0025 - val_loss: 23.3896 - val_acc: 0.0050\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(0.01), loss='mean_squared_error', metrics=['acc'])\n",
    "history = model.fit(x=all_feat_reshaped, y=all_label, batch_size=8, epochs=4, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input features: \n",
      " [[1. 4. 4.]\n",
      " [3. 6. 3.]\n",
      " [6. 6. 7.]\n",
      " [1. 8. 0.]\n",
      " [4. 0. 8.]]\n",
      "\n",
      "Labels: \n",
      " [[ 9.]\n",
      " [12.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [12.]]\n",
      "\n",
      "Predictions: \n",
      " [[10.375401]\n",
      " [12.513451]\n",
      " [12.754671]\n",
      " [ 8.693607]\n",
      " [12.877496]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nInput features: \\n', all_feat_reshaped[-5:,:])\n",
    "print('\\nLabels: \\n', all_label[-5:,:])\n",
    "print('\\nPredictions: \\n', model.predict(all_feat_reshaped[-5:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we need to check embedding weight too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_layer = model.get_layer('Embedding_Layer')\n",
    "embd_mats = embd_layer.get_weights()\n",
    "\n",
    "wgt_layer = model.get_layer('RNN_Layer')\n",
    "wgts_mats = wgt_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer should have size = $10 \\times 10$, as we're mapping 10 numbers(integers to be precise) to 10 dimensional vectors (1 vector for each of the number). In the weight matrix, index indicates the integer to which it is mapped.\n",
    "\n",
    "RNN weight shapes will be similar to the previous excerxise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding W shape:  (10, 10)\n",
      "W shape:  (10, 1)\n",
      "U shape:  (1, 1)\n",
      "b shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "print('Embedding W shape: ', embd_mats[0].shape)\n",
    "print('W shape: ', wgts_mats[0].shape)\n",
    "print('U shape: ', wgts_mats[1].shape)\n",
    "print('b shape: ', wgts_mats[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.36289757,  0.40314817, -0.37529683, -0.40290484,  0.39402398,\n",
       "          0.3687898 ,  0.4199001 ,  0.40679476,  0.3741078 ,  0.359894  ],\n",
       "        [-0.35347167,  0.3330296 , -0.39868364, -0.33981174,  0.3365241 ,\n",
       "          0.3281662 ,  0.3660595 ,  0.4048839 ,  0.34692815,  0.38161314],\n",
       "        [-0.45016566,  0.42293632, -0.42147616, -0.43580624,  0.4297624 ,\n",
       "          0.45729828,  0.44733754,  0.4054712 ,  0.46323246,  0.38260564],\n",
       "        [-0.5018592 ,  0.50228935, -0.4505884 , -0.5009186 ,  0.49097726,\n",
       "          0.47848928,  0.47305825,  0.46186924,  0.47454175,  0.44277638],\n",
       "        [-0.40599436,  0.43200484, -0.46300104, -0.4639299 ,  0.43344742,\n",
       "          0.46506122,  0.40431377,  0.42332262,  0.45043844,  0.42279685],\n",
       "        [-0.43375313,  0.43648008, -0.45081672, -0.44851768,  0.47521338,\n",
       "          0.47410107,  0.50128907,  0.49491104,  0.4410805 ,  0.4213932 ],\n",
       "        [-0.4954618 ,  0.46851364, -0.5116735 , -0.45529932,  0.47856602,\n",
       "          0.45150945,  0.4934331 ,  0.4998772 ,  0.48619613,  0.471267  ],\n",
       "        [-0.61349857,  0.5925937 , -0.54296917, -0.58609366,  0.59776765,\n",
       "          0.5471009 ,  0.5815874 ,  0.5709826 ,  0.5549795 ,  0.5045419 ],\n",
       "        [-0.65153235,  0.6257838 , -0.57797474, -0.6547753 ,  0.65709764,\n",
       "          0.64131314,  0.6108394 ,  0.62611336,  0.66329384,  0.51571363],\n",
       "        [-0.6335748 ,  0.645195  , -0.6012711 , -0.6162761 ,  0.66593677,\n",
       "          0.68635774,  0.6636241 ,  0.6305617 ,  0.64609253,  0.5530438 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.38934448],\n",
       "        [ 0.57726926],\n",
       "        [-0.16260014],\n",
       "        [-0.28410706],\n",
       "        [ 0.3671195 ],\n",
       "        [ 0.34153858],\n",
       "        [ 0.3436649 ],\n",
       "        [ 0.2549568 ],\n",
       "        [ 0.5274176 ],\n",
       "        [ 0.09343947]], dtype=float32),\n",
       " array([[-2.2878616]], dtype=float32),\n",
       " array([1.5465231], dtype=float32)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only $U$ makes the sense. Remember the RNN equaition:\n",
    "\n",
    "$h_t = f(X \\times W +  h_{t-1} \\times U + b)$\n",
    "\n",
    "Here, $X$ is the embedding output. Let's do one more transformation:  $ W_{embd} \\times W + b$, this will give us a number a vector containing 10 numbers, each corresponding to input number.\n",
    "\n",
    "Let's do it one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2990777],\n",
       "       [1.1745284],\n",
       "       [1.4629037],\n",
       "       [1.6165873],\n",
       "       [1.4564604],\n",
       "       [1.5284245],\n",
       "       [1.6033008],\n",
       "       [1.927354 ],\n",
       "       [2.1227646],\n",
       "       [2.1521482]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embd_mats[0], wgts_mats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8456008],\n",
       "       [2.7210515],\n",
       "       [3.0094268],\n",
       "       [3.1631103],\n",
       "       [3.0029836],\n",
       "       [3.0749476],\n",
       "       [3.149824 ],\n",
       "       [3.473877 ],\n",
       "       [3.6692877],\n",
       "       [3.6986713]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[2.8456008]\n",
      " [2.7210515]\n",
      " [3.0094268]\n",
      " [3.1631103]\n",
      " [3.0029836]\n",
      " [3.0749476]\n",
      " [3.149824 ]\n",
      " [3.473877 ]\n",
      " [3.6692877]\n",
      " [3.6986713]]\n",
      "\n",
      "U: \n",
      " [[-2.2878616]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2])\n",
    "print('\\nU: \\n', wgts_mats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes some sense, right!\n",
    "\n",
    "Let's round it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " W_embd * W + b: \n",
      " [[3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]]\n",
      "\n",
      "U: \n",
      " [[-2.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n W_embd * W + b: \\n', np.round(np.matmul(embd_mats[0], wgts_mats[0]) + wgts_mats[2]))\n",
    "print('\\nU: \\n', np.round(wgts_mats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of what happened:\n",
    "\n",
    "When you input an integer to ebmedding layer,it gives out a vector at corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36289757,  0.40314817, -0.37529683, -0.40290484,  0.39402398,\n",
       "         0.3687898 ,  0.4199001 ,  0.40679476,  0.3741078 ,  0.359894  ],\n",
       "       [-0.35347167,  0.3330296 , -0.39868364, -0.33981174,  0.3365241 ,\n",
       "         0.3281662 ,  0.3660595 ,  0.4048839 ,  0.34692815,  0.38161314],\n",
       "       [-0.45016566,  0.42293632, -0.42147616, -0.43580624,  0.4297624 ,\n",
       "         0.45729828,  0.44733754,  0.4054712 ,  0.46323246,  0.38260564],\n",
       "       [-0.5018592 ,  0.50228935, -0.4505884 , -0.5009186 ,  0.49097726,\n",
       "         0.47848928,  0.47305825,  0.46186924,  0.47454175,  0.44277638],\n",
       "       [-0.40599436,  0.43200484, -0.46300104, -0.4639299 ,  0.43344742,\n",
       "         0.46506122,  0.40431377,  0.42332262,  0.45043844,  0.42279685],\n",
       "       [-0.43375313,  0.43648008, -0.45081672, -0.44851768,  0.47521338,\n",
       "         0.47410107,  0.50128907,  0.49491104,  0.4410805 ,  0.4213932 ],\n",
       "       [-0.4954618 ,  0.46851364, -0.5116735 , -0.45529932,  0.47856602,\n",
       "         0.45150945,  0.4934331 ,  0.4998772 ,  0.48619613,  0.471267  ],\n",
       "       [-0.61349857,  0.5925937 , -0.54296917, -0.58609366,  0.59776765,\n",
       "         0.5471009 ,  0.5815874 ,  0.5709826 ,  0.5549795 ,  0.5045419 ],\n",
       "       [-0.65153235,  0.6257838 , -0.57797474, -0.6547753 ,  0.65709764,\n",
       "         0.64131314,  0.6108394 ,  0.62611336,  0.66329384,  0.51571363],\n",
       "       [-0.6335748 ,  0.645195  , -0.6012711 , -0.6162761 ,  0.66593677,\n",
       "         0.68635774,  0.6636241 ,  0.6305617 ,  0.64609253,  0.5530438 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_mats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43375313,  0.43648008, -0.45081672, -0.44851768,  0.47521338,\n",
       "        0.47410107,  0.50128907,  0.49491104,  0.4410805 ,  0.4213932 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In input was '5', output will be\n",
    "embd_mats[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This input is similar to one-hot encoding. \n",
    "\n",
    "In the next step(RNN), this vector get multipled to $W$ to produce a vector of rnn_size, which in this case is 1, so it gives out one number in our case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could see, embeddings learn represetation in combination to other matrices and thus might be difficult to explain directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
